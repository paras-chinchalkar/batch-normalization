# batch-normalization
Batch normalization (BatchNorm) is a technique used in neural networks to improve training speed, stability, and performance. It normalizes the inputs to each layer, helping to mitigate issues like internal covariate shift (i.e., changes in the distribution of activations during training). How Batch Normalization Works
